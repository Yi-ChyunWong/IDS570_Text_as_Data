library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
# Load text files ----
# You will need the correct file paths if you don't follow my naming conventions:
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
# Read the raw text files into R
text_a <- read_file(file_a)
text_b <- read_file(file_b)
# Combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
# Task 1: Diagnostics ----
count_tokens <- function(text) {
tibble(text = text) %>%
unnest_tokens(word, text) %>%
nrow()
}
count_word_types <- function(text) {
tibble(text = text) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
distinct(word) %>%
nrow()
}
corpus_diagnostics <- tibble(
doc_title = c("Text A", "Text B"),
n_chars = c(str_length(text_a), str_length(text_b)),
n_word_tokens = c(count_tokens(text_a),
count_tokens(text_b)),
n_word_types = c(count_word_types(text_a),
count_word_types(text_b))
)
corpus_diagnostics
# Task 2: Normalisation ----
# Start with tidytext's built-in stopword list
data("stop_words")
# Add our own project-specific stopwords (you can, and will, expand this list later)
custom_stopwords <- tibble(
word = c(
"vnto", "haue", "doo", "hath", "bee", "ye", "thee"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
all_stopwords %>% slice(1:10)
# Word count across documents
word_counts <- texts %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
word_counts
# Word count after stopword removal
doc_lengths <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lengths
# Normalised word count
word_counts_normalized <- word_counts %>%
left_join(doc_lengths, by = "doc_title") %>%
mutate(relative_freq = n / total_words)
word_counts_normalized
# "Trade" raw count + frequencies
word_counts_filtered <- word_counts_normalized %>%
filter(word == "trade")
word_counts_filtered
# Task 3: Frequency Visualisation ----
plot_n_words <- 20  # you can change this as needed
# Select the most frequent words overall
word_comparison_tbl <- word_counts_normalized %>%
pivot_wider(
names_from = doc_title,
values_from = n,
values_fill = 0
) %>%
mutate(max_n = pmax(`Text A`, `Text B`)) %>%
arrange(desc(max_n))
word_comparison_tbl
word_plot_data <- word_comparison_tbl %>%
slice_head(n = plot_n_words) %>%
pivot_longer(
cols = c(`Text A`, `Text B`),
names_to = "doc_title",
values_to = "n"
) %>%
mutate(word = fct_reorder(word, n, .fun = max))
word_plot_data
ggplot(word_plot_data, aes(x = relative_freq, y = word)) + #black magic happens thanks to ggplot
geom_col() +
facet_wrap(~ doc_title, scales = "free_x") +
labs(
title = "Most relatively frequent words (stopwords removed)",
subtitle = paste0(
"Top ", plot_n_words,
" words by relative frequency across both texts"
),
x = "Relative word frequency",
y = NULL
) +
theme_minimal()
View(corpus_diagnostics)
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
corpus_diagnostics
# You will need the correct file paths if you don't follow my naming conventions:
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
# Read the raw text files into R
text_a <- read_file(file_a)
text_b <- read_file(file_b)
# Combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text)
corpus_diagnostics
corpus_diagnostics <- texts %>%
# mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text)
corpus_diagnostics
text
texts
corpus_diagnostics <- texts
#   mutate(n_chars = str_length(text)) %>%
#   unnest_tokens(word, text)
#
corpus_diagnostics
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text))
#   unnest_tokens(word, text)
#
corpus_diagnostics
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, str_to_lower(text))
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word))
corpus_diagnostics
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
group_by(doc_title)
corpus_diagnostics
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
group_by(doc_title) %>%
summarise(
n_chars = n_chars,
n_word_tokens = n(word),
n_word_types = n_distinct(word)
)
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
group_by(doc_title) %>%
summarise(
n_chars = first(n_chars),
n_word_tokens = n(word),
n_word_types = n_distinct(word)
)
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
group_by(doc_title) %>%
summarise(
n_chars = first(n_chars),
n_word_tokens = n(),
n_word_types = n_distinct(word)
)
corpus_diagnostics
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
group_by(doc_title) %>%
summarise(
n_chars = first(n_chars),
n_word_tokens = n(),
n_word_types = n_distinct(word),
.groups = "drop"
)
corpus_diagnostics
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
group_by(doc_title) %>%
summarise(
n_chars = first(n_chars),
n_word_tokens = n(),
n_word_types = n_distinct(word)
)
corpus_diagnostics
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
#check they are there
print(texts)
data("stop_words")
custom_stopwords <- tibble(
word = c("vnto", "haue", "doo", "hath", "bee", "ye", "thee")
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
# I) Corpus diagnostics (BEFORE stopword removal)
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
group_by(doc_title) %>%
summarise(
n_chars = first(n_chars),
n_word_tokens = n(),
n_word_types = n_distinct(word),
.groups = "drop"
)
corpus_diagnostics
# Load libraries ----
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
# Load text files ----
# You will need the correct file paths if you don't follow my naming conventions:
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
# Read the raw text files into R
text_a <- read_file(file_a)
text_b <- read_file(file_b)
# Combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
# Task 1: Diagnostics ----
# count_tokens <- function(text) {
#   tibble(text = text) %>%
#     unnest_tokens(word, text) %>%
#     nrow()
# }
#
# count_word_types <- function(text) {
#   tibble(text = text) %>%
#     unnest_tokens(word, text) %>%
#     mutate(word = str_to_lower(word)) %>%
#     distinct(word) %>%
#     nrow()
# }
#
# corpus_diagnostics <- tibble(
#   doc_title = c("Text A", "Text B"),
#   n_chars = c(str_length(text_a), str_length(text_b)),
#   n_word_tokens = c(count_tokens(text_a),
#                     count_tokens(text_b)),
#   n_word_types = c(count_word_types(text_a),
#                    count_word_types(text_b))
# )
corpus_diagnostics <- texts %>%
mutate(n_chars = str_length(text)) %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
group_by(doc_title) %>%
summarise(
n_chars = first(n_chars),
n_word_tokens = n(),
n_word_types = n_distinct(word)
)
corpus_diagnostics
# Task 2: Normalisation ----
# Start with tidytext's built-in stopword list
data("stop_words")
# Add our own project-specific stopwords (you can, and will, expand this list later)
custom_stopwords <- tibble(
word = c(
"vnto", "haue", "doo", "hath", "bee", "ye", "thee"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
all_stopwords %>% slice(1:10)
# Word count across documents
word_counts <- texts %>%
unnest_tokens(word, text) %>%
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
word_counts
# Word count after stopword removal
doc_lengths <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lengths
# Normalised word count
word_counts_normalized <- word_counts %>%
left_join(doc_lengths, by = "doc_title") %>%
mutate(relative_freq = n / total_words)
word_counts_normalized
# "Trade" raw count + frequencies
word_counts_filtered <- word_counts_normalized %>%
filter(word == "trade")
word_counts_filtered
# Task 3: Frequency Visualisation ----
plot_n_words <- 20  # you can change this as needed
# Select the most frequent words overall
word_comparison_tbl <- word_counts_normalized %>%
pivot_wider(
names_from = doc_title,
values_from = n,
values_fill = 0
) %>%
mutate(max_n = pmax(`Text A`, `Text B`)) %>%
arrange(desc(max_n))
word_comparison_tbl
word_plot_data <- word_comparison_tbl %>%
slice_head(n = plot_n_words) %>%
pivot_longer(
cols = c(`Text A`, `Text B`),
names_to = "doc_title",
values_to = "n"
) %>%
mutate(word = fct_reorder(word, n, .fun = max))
word_plot_data
ggplot(word_plot_data, aes(x = relative_freq, y = word)) + #black magic happens thanks to ggplot
geom_col() +
facet_wrap(~ doc_title, scales = "free_x") +
labs(
title = "Most relatively frequent words (stopwords removed)",
subtitle = paste0(
"Top ", plot_n_words,
" words by relative frequency across both texts"
),
x = "Relative word frequency",
y = NULL
) +
theme_minimal()
